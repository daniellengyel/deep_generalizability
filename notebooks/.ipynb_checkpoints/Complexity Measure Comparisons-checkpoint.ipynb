{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle,os, copy\n",
    "os.environ[\"PATH_TO_DEEP_FOLDER\"] = \"/rds/general/user/dl2119/home/deep_generalizability\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "import re\n",
    "\n",
    "import margin_flatness as mf\n",
    "import margin_flatness.postprocessing as mf_post\n",
    "\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (13,9)\n",
    "\n",
    "COLORS = plt.cm.tab20(np.arange(20))\n",
    "\n",
    "CORRECT_COLOR_IDX = 3\n",
    "INCORRECT_COLOR_IDX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = os.environ[\"PATH_TO_DEEP_FOLDER\"]\n",
    "data_name = \"CIFAR10\"\n",
    "\n",
    "exp_name = \"\"\n",
    "\n",
    "exp_name = \"LeNet_short\" #\"May19_00-32-38_cx3-7-9.cx3.hpc.ic.ac.uk\"\n",
    "experiment_folder = os.path.join(root_folder, \"experiments\", data_name, exp_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = mf_post.stats_plotting.get_end_stats(experiment_folder, step=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.sort_values(by=\"Acc Test Mean\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cached data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mf.save_load.join_cached_sub_data(experiment_folder, \"point_traces\", -1, \"May18_23-54-38\")\n",
    "# mf.save_load.join_cached_sub_data(experiment_folder, \"point_loss\", -1, \"May18_23-54-28\")\n",
    "# mf.save_load.join_cached_sub_data(experiment_folder, \"inp_out_jacobian\", -1, \"May18_23-58-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_point_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_point_traces, _ = mf.save_load.load_cached_data(experiment_folder, \"point_traces\", step=-1, time_stamp=\"May25_16-21-58-862654-joined\")\n",
    "all_point_loss, _ = mf.save_load.load_cached_data(experiment_folder, \"point_loss\", step=-1, time_stamp=\"May25_17-48-36-470248-joined\")\n",
    "inp_out_jacobian, _ = mf.save_load.load_cached_data(experiment_folder, \"inp_out_jacobian\", step=-1, time_stamp=\"May25_16-19-33-203813-joined\")\n",
    "output_margins, _ = mf.save_load.load_cached_data(experiment_folder, \"output_margins\", step=-1, time_stamp=\"May25_15-00-54-625745-joined\")\n",
    "\n",
    "post_dict = {\"traces\": all_point_traces, \"losses\": all_point_loss, \"jacobian\": inp_out_jacobian, \"margins\": output_margins}\n",
    "# post_dict = {\"traces\": all_point_traces, \"margins\": output_margins, \"jacobian\": inp_out_jacobian,}\n",
    "\n",
    "train_loss_acc, _ = mf.save_load.load_cached_data(experiment_folder, \"model_loss_acc\", step=-1, time_stamp=\"May25_14-28-14-776489-joined\")\n",
    "test_loss_acc, _ = mf.save_load.load_cached_data(experiment_folder, \"model_loss_acc\", step=-1, time_stamp=\"May25_14-28-14-733072-joined\")\n",
    "loss_acc_dict = {k: {\"acc_train\": train_loss_acc[k][1]['0'], \"acc_test\": test_loss_acc[k][1]['0'], \"loss_train\": train_loss_acc[k][0]['0'], \"loss_test\": test_loss_acc[k][0]['0']} for k in train_loss_acc}\n",
    "loss_acc_df = pd.DataFrame(data=loss_acc_dict.values(), index=loss_acc_dict.keys())\n",
    "\n",
    "# loss_acc_dict = {losses: }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get processed DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_kendalls(exp_post_dict, exp_ids, configs):\n",
    "    kendall_coeffs = {}\n",
    "    \n",
    "    for k1 in exp_post_dict.keys():\n",
    "        for k2 in exp_post_dict.keys():\n",
    "            # We continue since kendall is symmetric\n",
    "            if (k2, k1) in kendall_coeffs or k1 == k2:\n",
    "                continue\n",
    "            kendall_coeffs[(k1, k2)] = {}\n",
    "\n",
    "            for exp_id in exp_ids:\n",
    "                kendall_coeffs[(k1, k2)][exp_id] = mf_post.correlation.get_kendall(exp_post_dict[k1][exp_id]['0'], exp_post_dict[k2][exp_id]['0'], remove_outliers=True).correlation\n",
    "    \n",
    "    hp_df = get_hp_df(configs)        \n",
    "    kendall_coeffs_df = pd.DataFrame(kendall_coeffs)\n",
    "    return pd.concat([kendall_coeffs_df, hp_df], axis=1)\n",
    "\n",
    "\n",
    "def get_all_r2(exp_post_dict, exp_ids, configs):\n",
    "    r2_coeffs = {}\n",
    "    \n",
    "    for k1 in exp_post_dict.keys():\n",
    "        for k2 in exp_post_dict.keys():\n",
    "            # We continue only if equal. r2 is not symmetric, so we get the max\n",
    "            if k1 == k2:\n",
    "                continue\n",
    "            \n",
    "            if (k1, k2) not in r2_coeffs:\n",
    "                r2_coeffs[(k1, k2)] = {}\n",
    "\n",
    "            for exp_id in exp_ids:\n",
    "                if (k2, k1) in r2_coeffs:\n",
    "                    try:\n",
    "                        r2_coeffs[(k2, k1)][exp_id] = max(r2_coeffs[(k2, k1)][exp_id], mf_post.correlation.get_isotonic_r_squared(exp_post_dict[k1][exp_id]['0'], exp_post_dict[k2][exp_id]['0'], remove_outliers=True, increasing=\"auto\"))\n",
    "                    except:\n",
    "                        pass\n",
    "                else:\n",
    "                    try:\n",
    "                        r2_coeffs[(k1, k2)][exp_id] = mf_post.correlation.get_isotonic_r_squared(exp_post_dict[k1][exp_id]['0'], exp_post_dict[k2][exp_id]['0'], remove_outliers=True, increasing=\"auto\") \n",
    "                    except:\n",
    "                        r2_coeffs[(k1, k2)][exp_id] = np.nan\n",
    "            \n",
    "    hp_df = get_hp_df(configs)        \n",
    "    r2_coeffs_df = pd.DataFrame(r2_coeffs)\n",
    "    return pd.concat([r2_coeffs_df, hp_df], axis=1)\n",
    "\n",
    "\n",
    "def get_hp_df(configs):\n",
    "    cfs_hp = mf_post.utils.get_hp(configs)\n",
    "    cfs_hp_df = configs[list(cfs_hp.keys())]\n",
    "    return cfs_hp_df\n",
    "\n",
    "cfgs = mf.save_load.load_configs(experiment_folder)\n",
    "exp_ids = list(cfgs.index)\n",
    "# exp_ids = [\"1621270824.0509984\"]\n",
    "kendall_df = get_all_kendalls(post_dict, exp_ids, cfgs)\n",
    "r2_df = get_all_r2(post_dict, exp_ids, cfgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_df(df, p_types, config_subset):\n",
    "    index_filter = [True] * len(df)\n",
    "    for k, v in config_subset.items():\n",
    "        index_filter = index_filter & (df[k] == v)\n",
    "            \n",
    "    if len(p_types) == 1:\n",
    "        return df[df.columns[[p_types[0] in c for c in df.columns]]][index_filter]\n",
    "    \n",
    "    col_filter = []\n",
    "    for c in df.columns:\n",
    "        if c[0] in p_types and c[1] in p_types:\n",
    "            col_filter.append(True)\n",
    "        else:\n",
    "            col_filter.append(False)\n",
    "    return df[df.columns[col_filter]][index_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kendall_df_new = pd.concat([kendall_df, loss_acc_df], axis=1)\n",
    "r2_df_new = pd.concat([r2_df, loss_acc_df], axis=1)\n",
    "\n",
    "# kendall_df_new[kendall_df_new[\"optimizer\"] == \"RMSProp\"].sort_values(\"train_loss\")\n",
    "# kendall_df_new.sort_values(\"train_loss\").tail(25)\n",
    "r2_df_new.groupby([\"criterion\", \"optimizer\", \"learning_rate\"]).mean().sort_values((\"margins\", \"jacobian\"))#.head(50)#[kendall_df_new[\"criterion\"] == \"cross-entropy\"].sort_values((\"traces\", \"losses\"), ascending=False).head(40) # .groupby([\"learning_rate\",\"optimizer\", \"criterion\"]).mean() #.loc[\"1621270824.0509984\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a histogram for kendall and isotonic R^2 value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "config_subset = {\"criterion\": \"cross-entropy\"} #, \"optimizer\": \"SGD\"}\n",
    "p_types = [\"losses\"]\n",
    "\n",
    "kendall_dict_to_use = subset_df(kendall_df, p_types, config_subset)\n",
    "r2_dict_to_use = subset_df(r2_df, p_types, config_subset)\n",
    "    \n",
    "    \n",
    "kendall_values = list(np.abs(kendall_dict_to_use.values.T))\n",
    "r2_values = list(r2_dict_to_use.values.T)\n",
    "labels = list(kendall_dict_to_use.columns)\n",
    "    \n",
    "sns.set(style=\"ticks\")\n",
    "f, (ax_kendall_hist, ax_r2_hist) = plt.subplots(2, figsize=(16,12))\n",
    "\n",
    "ax_kendall_hist.hist(kendall_values, rwidth=10, bins=np.linspace(-1, 1, 50), alpha=1, label=[\"Kendall: {} {}\".format(l[0], l[1]) for l in labels])\n",
    "ax_kendall_hist.tick_params(axis='both', labelbottom=True, labelsize=30)\n",
    "ax_kendall_hist.set_ylabel(ylabel=\"Count\", fontsize=35)\n",
    "ax_kendall_hist.grid(b=True, which='major')\n",
    "\n",
    "ax_r2_hist.hist(r2_values, rwidth=10, bins=np.linspace(-1, 1, 50), alpha=1, label=[\"R2: {} {}\".format(l[0], l[1]) for l in labels]) # TODO: Messed up due to Nan values. \n",
    "ax_r2_hist.tick_params(axis='both', labelbottom=True, labelsize=30)\n",
    "ax_r2_hist.set_ylabel(ylabel=\"Count\", fontsize=35)\n",
    "ax_r2_hist.grid(b=True, which='major')\n",
    "\n",
    "# ax_r2_hist.set_xlim(-0.2, 0.2)\n",
    "# ax_kendall_hist.set_xlim(-0.2, 0.2)\n",
    "\n",
    "ax_kendall_hist.legend(loc=\"upper left\", fontsize=27) #, bbox_to_anchor=(0.45, 0.95))\n",
    "ax_r2_hist.legend(loc=\"upper left\", fontsize=27) #, bbox_to_anchor=(0.3875, 0.95))\n",
    "\n",
    "\n",
    "# f.savefig(\"../figs/kendall_r2_{}_{}\".format(criterion_type, p_types), dpi=300, bbox_inches = \"tight\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "criterion_type = \"MSE\"\n",
    "p_types = [\"traces\"]\n",
    "\n",
    "kendall_dict_to_use = subset_df(kendall_df, p_types, criterion_type)\n",
    "r2_dict_to_use = subset_df(r2_df, p_types, criterion_type)\n",
    "    \n",
    "    \n",
    "kendall_values = list(np.abs(kendall_dict_to_use.values.T))\n",
    "r2_values = list(r2_dict_to_use.values.T)\n",
    "labels = list(kendall_dict_to_use.columns)\n",
    "    \n",
    "sns.set(style=\"ticks\")\n",
    "f, (ax_kendall_hist, ax_r2_hist) = plt.subplots(2, figsize=(16,12))\n",
    "\n",
    "ax_kendall_hist.hist(kendall_values, rwidth=10, bins=np.linspace(-1, 1, 50), alpha=1, label=[\"Kendall: {} {}\".format(l[0], l[1]) for l in labels])\n",
    "ax_kendall_hist.tick_params(axis='both', labelbottom=True, labelsize=30)\n",
    "ax_kendall_hist.set_ylabel(ylabel=\"Count\", fontsize=35)\n",
    "ax_kendall_hist.grid(b=True, which='major')\n",
    "\n",
    "ax_r2_hist.hist(r2_values, rwidth=10, bins=np.linspace(-1, 1, 50), alpha=1, label=[\"R2: {} {}\".format(l[0], l[1]) for l in labels])\n",
    "ax_r2_hist.tick_params(axis='both', labelbottom=True, labelsize=30)\n",
    "ax_r2_hist.set_ylabel(ylabel=\"Count\", fontsize=35)\n",
    "ax_r2_hist.grid(b=True, which='major')\n",
    "\n",
    "# ax_r2_hist.set_xlim(-0.2, 0.2)\n",
    "# ax_kendall_hist.set_xlim(-0.2, 0.2)\n",
    "\n",
    "ax_kendall_hist.legend(loc=\"upper left\", fontsize=27) #, bbox_to_anchor=(0.45, 0.95))\n",
    "ax_r2_hist.legend(loc=\"upper left\", fontsize=27) #, bbox_to_anchor=(0.3875, 0.95))\n",
    "\n",
    "\n",
    "# f.savefig(\"../figs/kendall_r2_{}_{}\".format(criterion_type, p_types), dpi=300, bbox_inches = \"tight\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter Plot non-aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_df[r2_df[\"criterion\"] == \"MSE\"].sort_values((\"traces\", \"jacobian\"), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dict = post_dict\n",
    "\n",
    "x_type = \"losses\"\n",
    "y_type = \"jacobian\"\n",
    "exp_id = \"1621792489.7292588\"\n",
    "with_isotonic = False\n",
    "\n",
    "# print(np.mean(exp_dict[\"losses\"][exp_id]['0']))\n",
    "    \n",
    "x_data = exp_dict[x_type][exp_id]['0']\n",
    "y_data = exp_dict[y_type][exp_id]['0'] # exp_dict[y_type][exp_id]['0']\n",
    "    \n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "f, ax1 = plt.subplots(1, figsize=(16,12))\n",
    "sns.scatterplot(x=x_data, y=y_data, ax=ax1) #, label=\"Kendall Coeff: {:.2f}\".format(exp_kendall))\n",
    "if with_isotonic:\n",
    "    iso_reg = IsotonicRegression(increasing=\"auto\").fit(x_data, y_data)\n",
    "    y_predicted = iso_reg.predict(x_data)\n",
    "    sns.scatterplot(x=x_data, y=y_predicted, ax=ax1)\n",
    "    \n",
    "ax1.set_ylabel(ylabel=y_type, fontsize=22)\n",
    "ax1.set_xlabel(xlabel=x_type, fontsize=22)\n",
    "# ax1.set_ylim(-250, 3500)\n",
    "ax1.grid(b=True, which='major')\n",
    "# ax1.text(x=0.99, y=0.04, transform=ax1.transAxes, s=\"Kendall Coeff: {:.2f}\".format(exp_kendall) ,\\\n",
    "#          fontsize=22, verticalalignment='top', horizontalalignment='right',\\\n",
    "#         backgroundcolor='white', color=c2)\n",
    "\n",
    "\n",
    "# axins2 = inset_axes(ax1, width=4, height=4, loc=1)\n",
    "# sns.scatterplot(x=x_data, y=y_data, ax=axins2)\n",
    "\n",
    "# # sub region of the original image\n",
    "# x1, x2, y1, y2 = 10, 16, -0.6, 1\n",
    "# axins2.set_xlim(x1, x2)\n",
    "# axins2.set_ylim(y1, y2)\n",
    "\n",
    "# # draw a bbox of the region of the inset axes in the parent axes and\n",
    "# # connecting lines between the bbox and the inset axes area\n",
    "# mark_inset(ax1, axins2, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# f.savefig(\"../figs/subset_kendall_{}_{}_{}\".format(id_type, data_name, ce_exp), dpi=300, bbox_inches = \"tight\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter Plot : Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregation(exp_post_dict, exp_ids):\n",
    "    res_dict = {}\n",
    "    for k in exp_post_dict:\n",
    "        res_dict[k] = {\"mean\": {}, \"max\": {}, \"min\": {}}\n",
    "        for exp_id in exp_ids:\n",
    "            res_dict[k][\"mean\"][exp_id] = np.mean(exp_post_dict[k][exp_id]['0'])\n",
    "            res_dict[k][\"max\"][exp_id] = np.max(exp_post_dict[k][exp_id]['0'])\n",
    "            res_dict[k][\"min\"][exp_id] = np.min(exp_post_dict[k][exp_id]['0'])\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_post_data_vals(data_type, data_subtype, post_dict, acc_dict, exp_id):\n",
    "    exp_ids = list(acc_dict.keys())\n",
    "    assert data_type in post_dict or data_type == \"acc\"    \n",
    "    if \"acc\" != data_type:\n",
    "        return post_dict[data_type][data_subtype][exp_id]\n",
    "    else:\n",
    "        if x_subtype == \"test\":\n",
    "            return acc_dict[exp_id]['0'][1]\n",
    "        else:\n",
    "            return acc_dict[exp_id]['0'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dict = ce_dict\n",
    "exp_ids = ce_stats_df.index\n",
    "agg_dict = get_aggregation(exp_dict,  exp_ids)\n",
    "acc_dict = ce_acc\n",
    "\n",
    "x_type = \"traces\"\n",
    "x_subtype = \"mean\"\n",
    "y_type = \"acc\"\n",
    "y_subtype = \"test\"\n",
    "\n",
    "with_isotonic = True\n",
    "\n",
    "\n",
    "x_data, y_data = [], []\n",
    "\n",
    "for exp_id in exp_ids:\n",
    "    x_data.append(get_post_data_vals(x_type, x_subtype, agg_dict, acc_dict, exp_id))\n",
    "    y_data.append(get_post_data_vals(y_type, y_subtype, agg_dict, acc_dict, exp_id))\n",
    "\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "f, ax1 = plt.subplots(1, figsize=(16,12))\n",
    "sns.scatterplot(x=x_data, y=y_data, ax=ax1) #, label=\"Kendall Coeff: {:.2f}\".format(exp_kendall))\n",
    "# if with_isotonic:\n",
    "#     iso_reg = IsotonicRegression(increasing=\"auto\").fit(x_data, y_data)\n",
    "#     y_predicted = iso_reg.predict(x_data)\n",
    "#     sns.scatterplot(x=x_data, y=y_predicted, ax=ax1)\n",
    "    \n",
    "ax1.set_ylabel(ylabel=y_type, fontsize=22)\n",
    "ax1.set_xlabel(xlabel=x_type, fontsize=22)\n",
    "# ax1.set_ylim(-250, 3500)\n",
    "ax1.grid(b=True, which='major')\n",
    "# ax1.text(x=0.99, y=0.04, transform=ax1.transAxes, s=\"Kendall Coeff: {:.2f}\".format(exp_kendall) ,\\\n",
    "#          fontsize=22, verticalalignment='top', horizontalalignment='right',\\\n",
    "#         backgroundcolor='white', color=c2)\n",
    "\n",
    "\n",
    "# f.savefig(\"../figs/subset_kendall_{}_{}_{}\".format(id_type, data_name, ce_exp), dpi=300, bbox_inches = \"tight\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot every point on one plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_points(exp_post_dict, exp_ids, x_type, y_type, use_isotonic=False, keep_kendall_threshold=0.3): \n",
    "    x_res_arr, y_res_arr = [], []\n",
    "    for exp_id in exp_ids:\n",
    "        x_data = exp_post_dict[x_type][exp_id]['0']\n",
    "        y_data = exp_post_dict[y_type][exp_id]['0']\n",
    "        if use_isotonic:\n",
    "            x_data, y_data = isotonic_outlier_removal(x_data, y_data, 0.02)\n",
    "            iso_reg = IsotonicRegression(increasing=\"auto\").fit(x_data, y_data)\n",
    "            y_predicted = iso_reg.predict(x_data)\n",
    "#             y_predicted += (x_data - np.min(x_data)) * 0.00000002 / np.linalg.norm(x_data) * np.linalg.norm(y_data)\n",
    "            x_data = y_predicted\n",
    "        if abs(mf_post.correlation.get_kendall(x_data, y_data).correlation) >= keep_kendall_threshold:\n",
    "            x_res_arr.append(x_data)\n",
    "            y_res_arr.append(y_data)\n",
    "    return np.array(x_res_arr).flatten(), np.array(y_res_arr).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.scatter(x_data, y_data)\n",
    "\n",
    "# iso_reg = IsotonicRegression(increasing=\"auto\").fit(x_data, y_data)\n",
    "# y_predicted = iso_reg.predict(x_data)\n",
    "# plt.scatter(x_data, y_predicted)\n",
    "\n",
    "# mf_post.correlation.linregress_outliers(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_points(exp_post_dict, exp_ids, x_type, y_type, use_isotonic=False, keep_kendall_threshold=0.3): \n",
    "    x_res_arr, y_res_arr = [], []\n",
    "    for exp_id in exp_ids:\n",
    "        x_data = exp_post_dict[x_type][exp_id]['0']\n",
    "        y_data = exp_post_dict[y_type][exp_id]['0']\n",
    "        if use_isotonic:\n",
    "            x_data, y_data = isotonic_outlier_removal(x_data, y_data, 0.02)\n",
    "            iso_reg = IsotonicRegression(increasing=\"auto\").fit(x_data, y_data)\n",
    "            y_predicted = iso_reg.predict(x_data)\n",
    "#             y_predicted += (x_data - np.min(x_data)) * 0.00000002 / np.linalg.norm(x_data) * np.linalg.norm(y_data)\n",
    "            x_data = y_predicted\n",
    "        if abs(mf_post.correlation.get_kendall(x_data, y_data).correlation) >= keep_kendall_threshold:\n",
    "            x_res_arr.append(x_data)\n",
    "            y_res_arr.append(y_data)\n",
    "    return np.array(x_res_arr).flatten(), np.array(y_res_arr).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_type = \"MSE\"\n",
    "x_type = \"traces\"\n",
    "y_type = \"jacobian\"\n",
    "do_isotonic = False\n",
    "\n",
    "if loss_type == \"CE\":\n",
    "    exp_name = ce_exp\n",
    "    exp_ids = ce_stats_df.index\n",
    "    x_data, y_data = get_all_points(ce_dict, exp_ids, x_type, y_type, use_isotonic=do_isotonic, keep_kendall_threshold=0)\n",
    "else:\n",
    "    exp_name = mse_exp\n",
    "    exp_ids = mse_stats_df.index\n",
    "    x_data, y_data = get_all_points(mse_dict, exp_ids, x_type, y_type, use_isotonic=do_isotonic, keep_kendall_threshold=-1)\n",
    "    \n",
    "c1 = [1.        , 0.49803922, 0.05490196, 1.        ]\n",
    "c2 = [0.12156863, 0.46666667, 0.70588235, 1.        ]\n",
    "\n",
    "\n",
    "f, ax1 = plt.subplots(1, figsize=(24,12))\n",
    "\n",
    "\n",
    "sns.scatterplot(x=x_data, y=y_data, ax=ax1, label=\"Kendall Coeff: {:.2f} \\nR2 score: {:.2f}\".format(mf_post.correlation.get_kendall(y_data, x_data).correlation, mf_post.correlation.get_isotonic_r_squared(y_data, x_data)))\n",
    "\n",
    "ax1.tick_params(axis='both', labelbottom=True, labelsize=30)\n",
    "\n",
    "ax1.set_ylabel(ylabel=y_type, fontsize=35)\n",
    "if do_isotonic:\n",
    "    ax1.set_xlabel(xlabel=\"g_theta({})\".format(x_type), fontsize=35)\n",
    "else:\n",
    "    ax1.set_xlabel(xlabel=\"{}\".format(x_type), fontsize=35)\n",
    "ax1.legend(loc=\"best\", fontsize=32) #, bbox_to_anchor=(0.3875, 0.95))\n",
    "ax1.grid(b=True, which='major')\n",
    "\n",
    "\n",
    "\n",
    "# ax1.text(x=0.99, y=0.9, transform=ax1.transAxes, s=\"Kendall Coeff: {:.2f} \\n R2 score: {:.2f}\".format(mf_post.correlation.get_kendall(y_data, x_data).correlation, mf_post.correlation.get_isotonic_r_squared(y_data, x_data)) ,\\\n",
    "#          fontsize=30, verticalalignment='top', horizontalalignment='right',\\\n",
    "#         backgroundcolor='white', color=c2)\n",
    "\n",
    "# iso_reg = IsotonicRegression(increasing=\"auto\").fit(x_data, y_data)\n",
    "# y_predicted = iso_reg.predict(x_data)\n",
    "# sns.scatterplot(x=x_data, y=y_predicted, ax=ax1, color=\"red\")\n",
    "\n",
    "\n",
    "if loss_type == \"CE\" and y_type == \"traces\":\n",
    "    axins2 = inset_axes(ax1, width=4, height=4, loc=2)\n",
    "    sns.scatterplot(x=x_data, y=y_data, ax=axins2)\n",
    "\n",
    "    # sub region of the original image\n",
    "    x1, x2, y1, y2 = -1000, 5000, -1000, 5000\n",
    "    \n",
    "    axins2.set_xlim(x1, x2)\n",
    "    axins2.set_ylim(y1, y2)\n",
    "\n",
    "    # draw a bbox of the region of the inset axes in the parent axes and\n",
    "    # connecting lines between the bbox and the inset axes area\n",
    "    mark_inset(ax1, axins2, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "#     axins2.tick_params(None) # TODO unset tick_params\n",
    "\n",
    "\n",
    "\n",
    "f.savefig(\"../figs/all_points_{}_{}_{}_{}\".format(loss_type, x_type, y_type, do_isotonic), dpi=300, bbox_inches = \"tight\",)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using with Isotonic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def get_outlier_filter(x_data, y_data):\n",
    "    x_data, y_data = np.array(x_data), np.array(y_data)\n",
    "    #     if len(x_data) < 5:\n",
    "    #         return np.array([True]*len(x_data)) # They are all outliers since we don't have enough datapoints\n",
    "    combined_data = np.concatenate([x_data.reshape(len(x_data), 1), y_data.reshape(len(y_data), 1)], axis=1)\n",
    "\n",
    "    clf = LocalOutlierFactor(n_neighbors=10, contamination=0.1)\n",
    "    outlier_filter = clf.fit_predict(combined_data) == 1\n",
    "\n",
    "       \n",
    "    return x_data[outlier_filter], y_data[outlier_filter]\n",
    "\n",
    "# get_outlier_filter(np.array(x_data), np.array(y_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot accuracy vs correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_all_point_traces, _ = mf.save_load.load_cached_data(ce_experiment_folder, \"point_traces\", step=-1, time_stamp=\"Dec02_14-40-03\")\n",
    "ce_all_point_output_margins, _ = mf.save_load.load_cached_data(ce_experiment_folder, \"output_margins\", step=-1, time_stamp=\"Dec02_20-51-39\")\n",
    "\n",
    "\n",
    "mse_all_point_traces, _ = mf.save_load.load_cached_data(mse_experiment_folder, \"point_traces\", step=-1, time_stamp=\"Dec02_14-40-03\")\n",
    "mse_all_point_output_margins, _ = mf.save_load.load_cached_data(mse_experiment_folder, \"output_margins\", step=-1, time_stamp=\"Dec02_18-53-57\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_nets = 1\n",
    "\n",
    "ce_kendall_coeffs_train_acc_dict = {}\n",
    "ce_kendall_coeffs_test_acc_dict = {}\n",
    "for exp_id in ce_stats_df.index:\n",
    "    for nn_idx in range(num_nets):\n",
    "        x_data = ce_all_point_traces[exp_id][\"{}\".format(nn_idx)]\n",
    "        y_data = ce_all_point_output_margins[exp_id][\"{}\".format(nn_idx)]\n",
    "        ce_kendall_coeffs_train_acc_dict[exp_id] = (mf_post.correlation.get_kendall(x_data, y_data, remove_outliers=True).correlation, ce_stats_df[\"Acc Train Mean\"][exp_id])\n",
    "        ce_kendall_coeffs_test_acc_dict[exp_id] = (mf_post.correlation.get_kendall(x_data, y_data, remove_outliers=True).correlation, ce_stats_df[\"Acc Test Mean\"][exp_id])\n",
    "\n",
    "mse_kendall_coeffs_train_acc_dict = {}\n",
    "mse_kendall_coeffs_test_acc_dict = {}\n",
    "for exp_id in mse_stats_df.index:\n",
    "    for nn_idx in range(num_nets):\n",
    "        x_data = mse_all_point_traces[exp_id][\"{}\".format(nn_idx)]\n",
    "        y_data = mse_all_point_output_margins[exp_id][\"{}\".format(nn_idx)]\n",
    "        mse_kendall_coeffs_train_acc_dict[exp_id] = (mf_post.correlation.get_kendall(x_data, y_data, remove_outliers=True).correlation, mse_stats_df[\"Acc Train Mean\"][exp_id])\n",
    "        mse_kendall_coeffs_test_acc_dict[exp_id] = (mf_post.correlation.get_kendall(x_data, y_data, remove_outliers=True).correlation, mse_stats_df[\"Acc Test Mean\"][exp_id])\n",
    "\n",
    "ce_kendall_coeffs_train_acc_arr = np.array(list(ce_kendall_coeffs_train_acc_dict.values()))\n",
    "mse_kendall_coeffs_train_acc_arr = np.array(list(mse_kendall_coeffs_train_acc_dict.values()))\n",
    "ce_kendall_coeffs_test_acc_arr = np.array(list(ce_kendall_coeffs_test_acc_dict.values()))\n",
    "mse_kendall_coeffs_test_acc_arr = np.array(list(mse_kendall_coeffs_test_acc_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "f, (ax1_hist) = plt.subplots(1, figsize=(16,12))\n",
    "\n",
    "ax1_hist.hist([ce_kendall_coeffs_train_acc_arr[:, 1], mse_kendall_coeffs_train_acc_arr[:, 1]], color=['darkblue','orange'], alpha=0.5, label=[\"Cross Entropy\", \"MSE\"])\n",
    "\n",
    "ax1_hist.grid(b=True, which='major')\n",
    "\n",
    "ax1_hist.tick_params(axis='both', labelbottom=True, labelsize=18)\n",
    "\n",
    "ax1_hist.set_ylabel(ylabel=\"Training Accuracy\", fontsize=22)\n",
    "\n",
    "ax1_hist.set_ylabel(ylabel=\"Count\", fontsize=22)\n",
    "\n",
    "f.legend(loc=\"upper right\", fontsize=20, bbox_to_anchor=(0.35, 0.88))\n",
    "\n",
    "f.savefig(\"../figs/train_acc\", dpi=300, bbox_inches = \"tight\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "f, (ax1_hist) = plt.subplots(1, figsize=(16,12))\n",
    "\n",
    "ax1_hist.hist([ce_kendall_coeffs_test_acc_arr[:, 1], mse_kendall_coeffs_test_acc_arr[:, 1]], color=['darkblue','orange'], alpha=0.5, label=[\"Cross Entropy\", \"MSE\"])\n",
    "\n",
    "ax1_hist.grid(b=True, which='major')\n",
    "\n",
    "ax1_hist.tick_params(axis='both', labelbottom=True, labelsize=18)\n",
    "\n",
    "ax1_hist.set_ylabel(ylabel=\"Test Accuracy\", fontsize=22)\n",
    "\n",
    "ax1_hist.set_ylabel(ylabel=\"Count\", fontsize=22)\n",
    "\n",
    "f.legend(loc=\"upper right\", fontsize=20, bbox_to_anchor=(0.35, 0.88))\n",
    "\n",
    "f.savefig(\"../figs/test_acc\", dpi=300, bbox_inches = \"tight\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_nets = 1\n",
    "\n",
    "ce_r2_coeffs_point_output = {}\n",
    "for exp_id in ce_stats_df.index:\n",
    "    for nn_idx in range(num_nets):\n",
    "        x_data = np.array(ce_all_point_traces[exp_id][\"{}\".format(nn_idx)])\n",
    "        y_data = np.array(ce_all_point_output[exp_id][\"{}\".format(nn_idx)])[:, 0]\n",
    "        ce_r2_coeffs_point_output[exp_id] = mf_post.correlation.get_isotonic_r_squared(x_data, y_data, remove_outliers=True, increasing=\"auto\")\n",
    "\n",
    "\n",
    "ce_r2_coeffs_point_loss = {}\n",
    "for exp_id in ce_stats_df.index:\n",
    "    for nn_idx in range(num_nets):\n",
    "        x_data = ce_all_point_traces[exp_id][\"{}\".format(nn_idx)]\n",
    "        y_data = ce_all_point_loss[exp_id][\"{}\".format(nn_idx)]\n",
    "        ce_r2_coeffs_point_loss[exp_id] = mf_post.correlation.get_isotonic_r_squared(x_data, y_data, remove_outliers=True, increasing=\"auto\")\n",
    "\n",
    "        \n",
    "ce_r2_coeffs_output_margins = {}\n",
    "for exp_id in ce_stats_df.index:\n",
    "    for nn_idx in range(num_nets):\n",
    "        x_data = ce_all_point_traces[exp_id][\"{}\".format(nn_idx)]\n",
    "        y_data = ce_all_point_output_margins[exp_id][\"{}\".format(nn_idx)]\n",
    "        ce_r2_coeffs_output_margins[exp_id] = mf_post.correlation.get_isotonic_r_squared(x_data, y_data, remove_outliers=True, increasing=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_nets = 1\n",
    "\n",
    "ce_test_acc_hess = {}\n",
    "for exp_id in ce_stats_df.index:\n",
    "    for nn_idx in range(num_nets):\n",
    "        x_data = np.array(ce_all_point_traces[exp_id][\"{}\".format(nn_idx)])\n",
    "        y_data = np.array(ce_all_point_output[exp_id][\"{}\".format(nn_idx)])[:, 0]\n",
    "        ce_test_acc_hess[exp_id] = [ce_stats_df.loc[exp_id][\"Acc Test Mean\"], np.mean(ce_all_point_loss[exp_id][\"{}\".format(nn_idx)])]\n",
    "\n",
    "ce_test_acc_hess_arr = np.array(list(ce_test_acc_hess.values()))\n",
    "\n",
    "mse_test_acc_hess = {}\n",
    "for exp_id in mse_stats_df.index:\n",
    "    for nn_idx in range(num_nets):\n",
    "        x_data = np.array(mse_all_point_traces[exp_id][\"{}\".format(nn_idx)])\n",
    "#         y_data = np.array(mse_all_point_output[exp_id][\"{}\".format(nn_idx)])[:, 0]\n",
    "        mse_test_acc_hess[exp_id] = [mse_stats_df.loc[exp_id][\"Acc Test Mean\"], np.mean(mse_all_point_loss[exp_id][\"{}\".format(nn_idx)])]#mse_stats_df.loc[exp_id][\"Loss Train Mean\"]] #np.mean(ce_all_point_traces[exp_id][\"{}\".format(nn_idx)])]\n",
    "\n",
    "mse_test_acc_hess_arr = np.array(list(mse_test_acc_hess.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "c1 = [1.        , 0.49803922, 0.05490196, 1.        ]\n",
    "c2 = [0.12156863, 0.46666667, 0.70588235, 1.        ]\n",
    "\n",
    "\n",
    "f, ax1 = plt.subplots(1, figsize=(16,12))\n",
    "\n",
    "x_data = mse_test_acc_hess_arr[:, 1]\n",
    "y_data = mse_test_acc_hess_arr[:, 0]\n",
    "\n",
    "print(mf_post.correlation.get_kendall(x_data, y_data, remove_outliers=True).correlation)\n",
    "print(mf_post.correlation.get_isotonic_r_squared(x_data, y_data, remove_outliers=True, increasing=\"auto\"))\n",
    "sns.scatterplot(x=x_data, y=y_data, ax=ax1) #, label=\"Kendall Coeff: {:.2f}\".format(exp_kendall))\n",
    "\n",
    "# ax1.tick_params(axis='both', labelbottom=True, labelsize=18)\n",
    "\n",
    "ax1.set_ylabel(ylabel=\"Test Acc\", fontsize=22)\n",
    "ax1.set_xlabel(xlabel=\"Hessian Trace\", fontsize=22)\n",
    "# ax1.set_ylim(-250, 3500)\n",
    "\n",
    "\n",
    "# ax1.text(x=0.99, y=0.04, transform=ax1.transAxes, s=\"Kendall Coeff: {:.2f}\".format(exp_kendall) ,\\\n",
    "#          fontsize=22, verticalalignment='top', horizontalalignment='right',\\\n",
    "#         backgroundcolor='white', color=c2)\n",
    "\n",
    "ax1.grid(b=True, which='major')\n",
    "\n",
    "# axins2 = inset_axes(ax1, width=4, height=4, loc=1)\n",
    "# sns.scatterplot(x=x_data, y=y_data, ax=axins2)\n",
    "\n",
    "# # sub region of the original image\n",
    "# x1, x2, y1, y2 = 10, 16, -0.6, 1\n",
    "# axins2.set_xlim(x1, x2)\n",
    "# axins2.set_ylim(y1, y2)\n",
    "\n",
    "# # draw a bbox of the region of the inset axes in the parent axes and\n",
    "# # connecting lines between the bbox and the inset axes area\n",
    "# mark_inset(ax1, axins2, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "\n",
    "\n",
    "#f.legend(loc=\"upper right\", fontsize=20, bbox_to_anchor=(0.4, 0.88))\n",
    "\n",
    "\n",
    "# f.savefig(\"../figs/subset_kendall_{}_{}_{}\".format(id_type, data_name, ce_exp), dpi=300, bbox_inches = \"tight\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "c1 = [1.        , 0.49803922, 0.05490196, 1.        ]\n",
    "c2 = [0.12156863, 0.46666667, 0.70588235, 1.        ]\n",
    "\n",
    "\n",
    "f, ax1 = plt.subplots(1, figsize=(16,12))\n",
    "\n",
    "x_data = mse_test_acc_hess_arr[:, 1]\n",
    "y_data = mse_test_acc_hess_arr[:, 0]\n",
    "print(mf_post.correlation.get_kendall(x_data, y_data, remove_outliers=True).correlation)\n",
    "print(mf_post.correlation.get_isotonic_r_squared(x_data, y_data, remove_outliers=True, increasing=\"auto\"))\n",
    "sns.scatterplot(x=x_data, y=y_data, ax=ax1) #, label=\"Kendall Coeff: {:.2f}\".format(exp_kendall))\n",
    "\n",
    "# ax1.tick_params(axis='both', labelbottom=True, labelsize=18)\n",
    "\n",
    "ax1.set_ylabel(ylabel=\"Test Acc\", fontsize=22)\n",
    "ax1.set_xlabel(xlabel=\"Hessian Trace\", fontsize=22)\n",
    "# ax1.set_ylim(-250, 3500)\n",
    "\n",
    "\n",
    "# ax1.text(x=0.99, y=0.04, transform=ax1.transAxes, s=\"Kendall Coeff: {:.2f}\".format(exp_kendall) ,\\\n",
    "#          fontsize=22, verticalalignment='top', horizontalalignment='right',\\\n",
    "#         backgroundcolor='white', color=c2)\n",
    "\n",
    "ax1.grid(b=True, which='major')\n",
    "\n",
    "# axins2 = inset_axes(ax1, width=4, height=4, loc=1)\n",
    "# sns.scatterplot(x=x_data, y=y_data, ax=axins2)\n",
    "\n",
    "# # sub region of the original image\n",
    "# x1, x2, y1, y2 = 10, 16, -0.6, 1\n",
    "# axins2.set_xlim(x1, x2)\n",
    "# axins2.set_ylim(y1, y2)\n",
    "\n",
    "# # draw a bbox of the region of the inset axes in the parent axes and\n",
    "# # connecting lines between the bbox and the inset axes area\n",
    "# mark_inset(ax1, axins2, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "\n",
    "\n",
    "#f.legend(loc=\"upper right\", fontsize=20, bbox_to_anchor=(0.4, 0.88))\n",
    "\n",
    "\n",
    "# f.savefig(\"../figs/subset_kendall_{}_{}_{}\".format(id_type, data_name, ce_exp), dpi=300, bbox_inches = \"tight\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ce_sample_flatness_old, _ = mf.save_load.load_cached_data(ce_experiment_folder, \"sample_average_flatness_pointwise\", step=-1, time_stamp=\"Apr07_18-31-05\")\n",
    "ce_sample_flatness, _ = mf.save_load.load_cached_data(ce_experiment_folder, \"sample_average_flatness_pointwise\", step=-1, time_stamp=\"Apr08_01-23-18\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id = list(ce_sample_flatness_old.keys())[0]\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "c1 = [1.        , 0.49803922, 0.05490196, 1.        ]\n",
    "c2 = [0.12156863, 0.46666667, 0.70588235, 1.        ]\n",
    "\n",
    "\n",
    "f, ax1 = plt.subplots(1, figsize=(16,12))\n",
    "\n",
    "x_data = ce_sample_flatness[exp_id]['0']\n",
    "y_data = ce_all_point_traces[exp_id]['0']\n",
    "# y_data = ce_sample_flatness_old[exp_id]['0']\n",
    "print(mf_post.correlation.get_kendall(x_data, y_data, remove_outliers=True).correlation)\n",
    "print(mf_post.correlation.get_isotonic_r_squared(x_data, y_data, remove_outliers=True, increasing=\"auto\"))\n",
    "sns.scatterplot(x=x_data, y=y_data, ax=ax1) #, label=\"Kendall Coeff: {:.2f}\".format(exp_kendall))\n",
    "\n",
    "# ax1.tick_params(axis='both', labelbottom=True, labelsize=18)\n",
    "\n",
    "ax1.set_ylabel(ylabel=\"Test Acc\", fontsize=22)\n",
    "ax1.set_xlabel(xlabel=\"Hessian Trace\", fontsize=22)\n",
    "# ax1.set_ylim(-250, 3500)\n",
    "\n",
    "\n",
    "# ax1.text(x=0.99, y=0.04, transform=ax1.transAxes, s=\"Kendall Coeff: {:.2f}\".format(exp_kendall) ,\\\n",
    "#          fontsize=22, verticalalignment='top', horizontalalignment='right',\\\n",
    "#         backgroundcolor='white', color=c2)\n",
    "\n",
    "ax1.grid(b=True, which='major')\n",
    "\n",
    "# axins2 = inset_axes(ax1, width=4, height=4, loc=1)\n",
    "# sns.scatterplot(x=x_data, y=y_data, ax=axins2)\n",
    "\n",
    "# # sub region of the original image\n",
    "# x1, x2, y1, y2 = 10, 16, -0.6, 1\n",
    "# axins2.set_xlim(x1, x2)\n",
    "# axins2.set_ylim(y1, y2)\n",
    "\n",
    "# # draw a bbox of the region of the inset axes in the parent axes and\n",
    "# # connecting lines between the bbox and the inset axes area\n",
    "# mark_inset(ax1, axins2, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "\n",
    "\n",
    "#f.legend(loc=\"upper right\", fontsize=20, bbox_to_anchor=(0.4, 0.88))\n",
    "\n",
    "\n",
    "# f.savefig(\"../figs/subset_kendall_{}_{}_{}\".format(id_type, data_name, ce_exp), dpi=300, bbox_inches = \"tight\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_nets = 1\n",
    "\n",
    "ce_test_acc_hess = {}\n",
    "for exp_id in ce_stats_df.index:\n",
    "    for nn_idx in range(num_nets):\n",
    "        x_data = np.array(ce_sample_flatness[exp_id][\"{}\".format(nn_idx)])\n",
    "        y_data = np.array(ce_all_point_output[exp_id][\"{}\".format(nn_idx)])[:, 0]\n",
    "        ce_test_acc_hess[exp_id] = [ce_stats_df.loc[exp_id][\"Acc Test Mean\"], np.mean(ce_sample_flatness[exp_id][\"{}\".format(nn_idx)])]\n",
    "\n",
    "ce_test_acc_hess_arr = np.array(list(ce_test_acc_hess.values()))\n",
    "\n",
    "# mse_test_acc_hess = {}\n",
    "# for exp_id in mse_stats_df.index:\n",
    "#     for nn_idx in range(num_nets):\n",
    "#         x_data = np.array(mse_all_point_traces[exp_id][\"{}\".format(nn_idx)])\n",
    "# #         y_data = np.array(mse_all_point_output[exp_id][\"{}\".format(nn_idx)])[:, 0]\n",
    "#         mse_test_acc_hess[exp_id] = [mse_stats_df.loc[exp_id][\"Acc Test Mean\"], mse_stats_df.loc[exp_id][\"Loss Train Mean\"]] #np.mean(ce_all_point_traces[exp_id][\"{}\".format(nn_idx)])]\n",
    "\n",
    "# mse_test_acc_hess_arr = np.array(list(mse_test_acc_hess.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "c1 = [1.        , 0.49803922, 0.05490196, 1.        ]\n",
    "c2 = [0.12156863, 0.46666667, 0.70588235, 1.        ]\n",
    "\n",
    "\n",
    "f, ax1 = plt.subplots(1, figsize=(16,12))\n",
    "\n",
    "x_data = ce_test_acc_hess_arr[:, 1]\n",
    "y_data = ce_test_acc_hess_arr[:, 0]\n",
    "print(mf_post.correlation.get_kendall(x_data, y_data, remove_outliers=True).correlation)\n",
    "print(mf_post.correlation.get_isotonic_r_squared(x_data, y_data, remove_outliers=True, increasing=\"auto\"))\n",
    "sns.scatterplot(x=x_data, y=y_data, ax=ax1) #, label=\"Kendall Coeff: {:.2f}\".format(exp_kendall))\n",
    "\n",
    "# ax1.tick_params(axis='both', labelbottom=True, labelsize=18)\n",
    "\n",
    "ax1.set_ylabel(ylabel=\"Test Acc\", fontsize=22)\n",
    "ax1.set_xlabel(xlabel=\"Hessian Trace\", fontsize=22)\n",
    "# ax1.set_ylim(-250, 3500)\n",
    "\n",
    "\n",
    "# ax1.text(x=0.99, y=0.04, transform=ax1.transAxes, s=\"Kendall Coeff: {:.2f}\".format(exp_kendall) ,\\\n",
    "#          fontsize=22, verticalalignment='top', horizontalalignment='right',\\\n",
    "#         backgroundcolor='white', color=c2)\n",
    "\n",
    "ax1.grid(b=True, which='major')\n",
    "\n",
    "# axins2 = inset_axes(ax1, width=4, height=4, loc=1)\n",
    "# sns.scatterplot(x=x_data, y=y_data, ax=axins2)\n",
    "\n",
    "# # sub region of the original image\n",
    "# x1, x2, y1, y2 = 10, 16, -0.6, 1\n",
    "# axins2.set_xlim(x1, x2)\n",
    "# axins2.set_ylim(y1, y2)\n",
    "\n",
    "# # draw a bbox of the region of the inset axes in the parent axes and\n",
    "# # connecting lines between the bbox and the inset axes area\n",
    "# mark_inset(ax1, axins2, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "\n",
    "\n",
    "#f.legend(loc=\"upper right\", fontsize=20, bbox_to_anchor=(0.4, 0.88))\n",
    "\n",
    "\n",
    "# f.savefig(\"../figs/subset_kendall_{}_{}_{}\".format(id_type, data_name, ce_exp), dpi=300, bbox_inches = \"tight\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isotonic_outlier_removal(x_data, y_data, percent_outlier_remove=0.05, increasing=\"auto\"):\n",
    "    num_outlier_remove = int(len(x_data)*percent_outlier_remove)\n",
    "    for _ in range(num_outlier_remove):\n",
    "        iso_reg = IsotonicRegression(increasing=increasing).fit(x_data, y_data)\n",
    "        y_predicted = iso_reg.predict(x_data)\n",
    "        idx_max = np.argmax(np.abs(y_predicted - y_data))\n",
    "        x_data = np.delete(x_data, idx_max)\n",
    "        y_data = np.delete(y_data, idx_max)\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "def get_isotonic_r_squared(x_data, y_data, remove_outliers=False, increasing=False):\n",
    "    if len(x_data) == 0:\n",
    "        return 0, 0, 0, None, None\n",
    "    x_data, y_data = np.array(x_data), np.array(y_data)\n",
    "\n",
    "    if remove_outliers:\n",
    "        outlier_filter = get_outlier_filter(x_data, y_data)\n",
    "        x_data, y_data = x_data[outlier_filter], y_data[outlier_filter]\n",
    "        f\n",
    "    iso_reg = IsotonicRegression(increasing=increasing).fit(x_data, y_data)\n",
    "    y_predicted = iso_reg.predict(x_data)\n",
    "    return r2_score(y_data, y_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slopes = []\n",
    "weights = []\n",
    "\n",
    "ce_models = mf.save_load.get_all_models(ce_experiment_folder, step=-1, device=None)\n",
    "\n",
    "num_nets = 1\n",
    "\n",
    "ce_all_x_data = []\n",
    "ce_all_y_data = []\n",
    "counter = 0\n",
    "for exp_id in ce_stats_df.index:\n",
    "    if counter > 100:\n",
    "        break\n",
    "    counter += 1\n",
    "    for nn_idx in range(num_nets):\n",
    "        x_data = ce_all_point_traces[exp_id][\"{}\".format(nn_idx)]\n",
    "        y_data = ce_all_point_loss[exp_id][\"{}\".format(nn_idx)] #ce_all_point_output_margins[exp_id][\"{}\".format(nn_idx)]\n",
    "        x_data, y_data = isotonic_outlier_removal(x_data, y_data, 0.02)\n",
    "        iso_reg = IsotonicRegression(increasing=\"auto\").fit(x_data, y_data)\n",
    "        y_predicted = iso_reg.predict(x_data)\n",
    "        y_predicted_org = np.copy(y_predicted)\n",
    "        y_predicted += (x_data - np.min(x_data)) * 0.00002 / np.linalg.norm(x_data) * np.linalg.norm(y_data)\n",
    "\n",
    "        slope, intercept, correct_r_value, _, _ = mf_post.correlation.linregress_outliers(x_data, y_data)\n",
    "        y_lin_predicted = np.array(x_data) * float(slope) + intercept\n",
    "\n",
    "        slopes.append(slope)\n",
    "        weights.append(mf.utils.get_params_norm(ce_models[exp_id][\"0\"]))\n",
    "\n",
    "#         ce_all_x_data.append(np.mean(x_data))\n",
    "#         ce_all_y_data.append(np.max(y_data))\n",
    "        \n",
    "# ce_all_x_data = np.array(ce_all_x_data).flatten()\n",
    "# ce_all_y_data = np.array(ce_all_y_data).flatten()\n",
    "     \n",
    "# exp_name = ce_exp\n",
    "# x_data = ce_all_x_data\n",
    "# y_data = ce_all_y_data\n",
    "\n",
    "\n",
    "    \n",
    "#         c1 = [1.        , 0.49803922, 0.05490196, 1.        ]\n",
    "\n",
    "\n",
    "#         f, ax1 = plt.subplots(1, figsize=(16,12))\n",
    "        \n",
    "#         sns.scatterplot(x=y_lin_predicted, y=y_data)\n",
    "# #         sns.scatterplot(x=x_data, y=y_data)\n",
    "# #         sns.scatterplot(x=y_lin_predicted, y=y_data)\n",
    "# #         sns.scatterplot(x=y_predicted, y=y_data, ax=ax1) #, label=\"Kendall Coeff: {:.2f}\".format(exp_kendall))\n",
    "\n",
    "#         # ax1.tick_params(axis='both', labelbottom=True, labelsize=18)\n",
    "\n",
    "#         ax1.set_ylabel(ylabel=\"Hessian Trace\", fontsize=22)\n",
    "#         ax1.set_xlabel(xlabel=\"Output Margin\", fontsize=22)\n",
    "#         # ax1.set_ylim(-50, 300)\n",
    "        \n",
    "#         print(mf_post.correlation.get_kendall(y_predicted, y_data).correlation)\n",
    "#         print(mf_post.correlation.get_kendall(y_lin_predicted, y_data).correlation)\n",
    "\n",
    "#         print(mf_post.correlation.get_kendall(x_data, y_data).correlation)\n",
    "#         ax1.text(x=0.99, y=0.04, transform=ax1.transAxes, s=\"R2 Score: {:.2f}\".format(r2_score(y_data, y_lin_predicted)) ,\\\n",
    "#              fontsize=22, verticalalignment='top', horizontalalignment='right',\\\n",
    "#             backgroundcolor='white', color=c2)\n",
    "\n",
    "#         ax1.grid(b=True, which='major')\n",
    "#         plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(slopes, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_nets(m1, m2, N, inp_dim, seed=0):\n",
    "    x = torch.randn(N, inp_dim)\n",
    "    y1 = m1(x)\n",
    "    y2 = m2(x)\n",
    "    diff = torch.norm(y1 - y2, dim=1)\n",
    "    return float(torch.mean(diff).detach().numpy())\n",
    "\n",
    "m1 = ce_models[ce_stats_df.index[0]][\"0\"]\n",
    "m2 = ce_models[ce_stats_df.index[1]][\"0\"]\n",
    "\n",
    "N = 100\n",
    "inp_dim = 28*28\n",
    "\n",
    "# ce_stats_df.index[0]\n",
    "\n",
    "arr_dists = []\n",
    "for i in range(81):\n",
    "    for j in range(i, 81):\n",
    "        arr_dists.append(compare_nets(ce_models[ce_stats_df.index[i]][\"0\"], ce_models[ce_stats_df.index[j]][\"0\"], N, inp_dim, seed=0))\n",
    "\n",
    "# compare_nets(m1, m2, 1000, 28*28)\n",
    "\n",
    "\n",
    "# for exp_id in ce_stats_df.index:\n",
    "#     for exp_id in ce_stats_df.index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "seed = 0\n",
    "num_datapoints = 100\n",
    "\n",
    "# m1 = ce_models[ce_stats_df.index[0]][\"0\"]\n",
    "\n",
    "\n",
    "train_data, test_data = mf.postprocessing.utils.get_data_for_experiment(experiment_folder)\n",
    "data = mf.data_getters.get_random_data_subset(train_data, num_datapoints=num_datapoints, seed=seed)\n",
    "\n",
    "data_loader = DataLoader(data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.eval()\n",
    "\n",
    "res = []\n",
    "\n",
    "m_new = mf.nets.Nets.DanielNormOutputNet(m)\n",
    "\n",
    "for i, (inputs, labels) in enumerate(data_loader):\n",
    "\n",
    "    outputs = m_new(inputs) # get softmax of this \n",
    "    temp_res = []\n",
    "    soft_layer = torch.nn.Softmax(dim=-1)\n",
    "    for a in np.linspace(1, 10):\n",
    "        temp_res.append(max(soft_layer(a*outputs).detach().numpy()[0]))\n",
    "    res.append(temp_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_arr = []\n",
    "for i in range(len(res)):\n",
    "    for j in range(i+1, len(res)):\n",
    "        int_arr.append(do_intersect(res[i], res[j]))\n",
    "        if int_arr[-1]:\n",
    "            print(\"{} {}\".format(i, j))\n",
    "        \n",
    "# int_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(res[48])\n",
    "plt.plot(res[71])\n",
    "# plt.xlim(40, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_dists = np.array([np.array(dict_dists[i]) for i in range(81)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_intersect(res1, res2):\n",
    "    for i in range(len(res1)):\n",
    "        if res1[0] < res2[0]:\n",
    "            if res1[i] > res2[i]:\n",
    "                return True\n",
    "        if res1[0] > res2[0]:\n",
    "            if res1[i] < res2[i]:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "delta = 2\n",
    "for _ in range(100):\n",
    "    a = np.random.random(10)\n",
    "    a[0] = delta*max(a)\n",
    "\n",
    "\n",
    "    res1 = []\n",
    "    for i in np.linspace(0, 100):\n",
    "        o = np.exp(i*a) / np.sum(np.exp(i*a))\n",
    "        res1.append(o.dot(o))\n",
    "\n",
    "    res1 = np.array(res1)\n",
    "\n",
    "\n",
    "    a = np.random.random(10)\n",
    "    a[0] = delta*max(a)\n",
    "\n",
    "    res2 = []\n",
    "    for i in np.linspace(0, 100):\n",
    "        o = np.exp(i*a) / np.sum(np.exp(i*a))\n",
    "        res2.append(o.dot(o))\n",
    "\n",
    "    res2 = np.array(res2)\n",
    "    print(do_intersect(res1, res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os, pickle\n",
    "\n",
    "\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.KMNIST(\"/Users/daniellengyel/deep_generalizability/experiments/KMNIST\", train=True,\n",
    "                                            download=True,\n",
    "                                            transform=torchvision.transforms.Compose([\n",
    "                                                torchvision.transforms.ToTensor(),\n",
    "                                                torchvision.transforms.Normalize(\n",
    "                                                    (0.1307,), (0.3081,))\n",
    "                                            ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array(iter(DataLoader(train_data, batch_size=1, shuffle=True)).next()[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(d.reshape(28,28), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_type = \"best\"\n",
    "\n",
    "if id_type == \"worst\":\n",
    "    exp_id, exp_kendall = min(ce_kendall_coeffs.items(), key=lambda kv: abs(kv[1]))\n",
    "elif id_type == \"best\":\n",
    "    exp_id, exp_kendall = max(ce_kendall_coeffs.items(), key=lambda kv: abs(kv[1]))\n",
    "else:\n",
    "    exp_id, exp_kendall = sorted(ce_kendall_coeffs.items(), key=lambda kv: abs(kv[1]))[len(ce_kendall_coeffs)//2]\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "c1 = [1.        , 0.49803922, 0.05490196, 1.        ]\n",
    "c2 = [0.12156863, 0.46666667, 0.70588235, 1.        ]\n",
    "\n",
    "\n",
    "f, ax1 = plt.subplots(1, figsize=(16,12))\n",
    "\n",
    "x_data = ce_all_point_output_margins[exp_id][\"0\"]\n",
    "y_data = ce_all_point_traces[exp_id][\"0\"]\n",
    "sns.scatterplot(x=x_data, y=y_data, ax=ax1) #, label=\"Kendall Coeff: {:.2f}\".format(exp_kendall))\n",
    "\n",
    "# ax1.tick_params(axis='both', labelbottom=True, labelsize=18)\n",
    "\n",
    "ax1.set_ylabel(ylabel=\"Hessian Trace\", fontsize=22)\n",
    "ax1.set_xlabel(xlabel=\"Output Margin\", fontsize=22)\n",
    "ax1.set_ylim(-250, 3500)\n",
    "\n",
    "\n",
    "ax1.text(x=0.99, y=0.04, transform=ax1.transAxes, s=\"Kendall Coeff: {:.2f}\".format(exp_kendall) ,\\\n",
    "         fontsize=22, verticalalignment='top', horizontalalignment='right',\\\n",
    "        backgroundcolor='white', color=c2)\n",
    "\n",
    "ax1.grid(b=True, which='major')\n",
    "\n",
    "axins2 = inset_axes(ax1, width=4, height=4, loc=1)\n",
    "sns.scatterplot(x=x_data, y=y_data, ax=axins2)\n",
    "\n",
    "# sub region of the original image\n",
    "x1, x2, y1, y2 = 10, 16, -0.6, 1\n",
    "axins2.set_xlim(x1, x2)\n",
    "axins2.set_ylim(y1, y2)\n",
    "\n",
    "# draw a bbox of the region of the inset axes in the parent axes and\n",
    "# connecting lines between the bbox and the inset axes area\n",
    "mark_inset(ax1, axins2, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "\n",
    "\n",
    "#f.legend(loc=\"upper right\", fontsize=20, bbox_to_anchor=(0.4, 0.88))\n",
    "\n",
    "\n",
    "# f.savefig(\"../figs/subset_kendall_{}_{}_{}\".format(id_type, data_name, ce_exp), dpi=300, bbox_inches = \"tight\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3be6f8647cd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"criterion\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"cross-entropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"normalize_output\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtraces_CE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmf_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_on_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"point_traces\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_test_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"criterion\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"cross-entropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"normalize_output\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep_generalizability/margin_flatness/postprocessing/postprocess_experiment.py\u001b[0m in \u001b[0;36mcompute_on_experiment\u001b[0;34m(experiment_folder, name, exp_ids, step, seed, num_datapoints, on_test_set, device, verbose, check_cache, meta)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mmodels_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"meta\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeta_dict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"normalize_output\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeta_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmeta_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"normalize_output\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep_generalizability/margin_flatness/save_load.py\u001b[0m in \u001b[0;36mget_models\u001b[0;34m(model_folder_path, step, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mmodel_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_file_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mmodels_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep_generalizability/margin_flatness/save_load.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(PATH, device)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0mmeta_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_nets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep_generalizability/margin_flatness/training_utils.py\u001b[0m in \u001b[0;36mget_nets\u001b[0;34m(net_name, net_params, num_nets, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mnets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSimpleNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnet_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnet_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"LeNet\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mnets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLeNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnet_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnet_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"LinearNet\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLinearNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnet_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep_generalizability/margin_flatness/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mnets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSimpleNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnet_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnet_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"LeNet\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mnets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLeNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnet_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnet_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"LinearNet\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLinearNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnet_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep_generalizability/margin_flatness/nets/Nets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, height, width, channels, out_dim)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLeNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLeNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "root_folder = os.environ[\"PATH_TO_DEEP_FOLDER\"]\n",
    "data_name = \"CIFAR10\"\n",
    "\n",
    "exp_name = \"LeNet_short\"\n",
    "experiment_folder = os.path.join(root_folder, \"experiments\", data_name, exp_name)\n",
    "\n",
    "exp_ids = [\"1621787441.2034013\"]\n",
    "\n",
    "meta = {\"criterion\": \"cross-entropy\", \"normalize_output\": False}\n",
    "traces_CE = mf_post.postprocess_experiment.compute_on_experiment(experiment_folder, \"point_traces\", exp_ids, -1, 0, 10, on_test_set=False, device=None, verbose=True, check_cache=False, meta=meta)\n",
    "\n",
    "meta = {\"criterion\": \"cross-entropy\", \"normalize_output\": True}\n",
    "traces_new_norm_CE = mf_post.postprocess_experiment.compute_on_experiment(experiment_folder, \"point_traces\", exp_ids, -1, 0, 10, on_test_set=False, device=None, verbose=True, check_cache=False, meta=meta)\n",
    "\n",
    "# meta = {\"criterion\": \"MSE\"}\n",
    "# traces_MSE = mf_post.postprocess_experiment.compute_on_experiment(experiment_folder, \"point_traces\", exp_ids, -1, 0, 100, on_test_set=False, device=None, verbose=True, check_cache=False, meta=meta)\n",
    "\n",
    "# meta = {\"criterion\": \"normalized-cross-entropy\"}\n",
    "# traces_norm_CE = mf_post.postprocess_experiment.compute_on_experiment(experiment_folder, \"point_traces\", exp_ids, -1, 0, 100, on_test_set=False, device=None, verbose=True, check_cache=False, meta=meta)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = None # {\"criterion\": \"cross-entropy\"}\n",
    "jacob = mf_post.postprocess_experiment.compute_on_experiment(experiment_folder, \"inp_out_jacobian\", exp_ids, -1, 0, 100, on_test_set=False, device=None, verbose=True, check_cache=False, meta=meta)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margins = mf_post.postprocess_experiment.compute_on_experiment(experiment_folder, \"output_margins\", exp_ids, -1, 0, 100, on_test_set=False, device=None, verbose=True, check_cache=False, meta=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x152ef18bf220>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_id = exp_ids[0]\n",
    "x_data = traces_CE[exp_id]['0']\n",
    "y_data = traces_new_norm_CE[exp_id]['0']\n",
    "plt.scatter(x_data, y_data)\n",
    "# plt.xlim(-10, 20)\n",
    "# plt.ylim(-0.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dict\n",
    "exp_id = \"1621270447.8047616\"\n",
    "x_data = post_dict[\"losses\"][exp_id]['0']\n",
    "y_data = post_dict[\"jacobian\"][exp_id]['0']\n",
    "plt.scatter(x_data, y_data)\n",
    "# plt.xlim(-10, 20)\n",
    "# plt.ylim(-0.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_post.correlation.get_kendall(x_data, y_data, remove_outliers=False)#, increasing=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = mf_post.postprocess_experiment.get_exp_loss_acc(experiment_folder, -1, seed=0, num_train_datapoints=1000, num_test_datapoints=200, device=None, check_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(data=[loss[k]['0'] for k in loss], index=list(loss.keys()), columns=[\"train_loss\", \"test_loss\"])\n",
    "acc_df = pd.DataFrame(data=[acc[k]['0'] for k in acc], index=list(acc.keys()), columns=[\"train_acc\", \"test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "May19_00-32-38_cx3-7-9.cx3.hpc.ic.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = os.environ[\"PATH_TO_DEEP_FOLDER\"]\n",
    "data_name = \"KMNIST\"\n",
    "\n",
    "exp_name = \"\"\n",
    "\n",
    "exp_name = \"w128_l8_ReLU\" # \"May19_00-32-38_cx3-7-9.cx3.hpc.ic.ac.uk\" # \"May19_02-49-40_cx3-3-0.cx3.hpc.ic.ac.uk\"#\n",
    "experiment_folder = os.path.join(root_folder, \"experiments\", data_name, exp_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_gen",
   "language": "python",
   "name": "deep_gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
